import os

from dotenv import load_dotenv
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate

from openai import OpenAI

# Load environment variables from .env file
load_dotenv()

# Get the Telegram token and OpenAI API key from environment variables
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize the OpenAI client
client = OpenAI()

def handle_transcribe_openai(audio_file):
    """
    Function to transcribe audio using OpenAI.

    Parameters:
    audio_file (file): The audio file to be transcribed.

    Returns:
    str: The transcribed text.
    """
    # Transcribe the audio file using the 'whisper-1' model
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file
    )

    # Return the transcribed text
    return transcript.text

def handle_openai_model(chat_id, input_text, conversations, model_name):
    """
    Function to handle the OpenAI model.

    Parameters:
    chat_id (int): The chat ID.
    input_text (str): The input text to be processed by the model.
    conversations (dict): The conversations history.
    model_name (str): The name of the model to be used.

    Returns:
    str: The text generated by the model.
    """
    # Initialize the OpenAI model with the specified model name and configuration
    llm = ChatOpenAI(temperature=.0, model_name=model_name, verbose=False, model_kwargs={"stream": False},
                     openai_api_key=OPENAI_API_KEY)

    # Define the template for the prompt
    template = """
        {chat_history}
        Human: {human_input}
        Chatbot:"""

    # Initialize the prompt template with the defined template and input variables
    prompt_template = PromptTemplate(
        input_variables=["chat_history", "human_input"], template=template
    )

    # Initialize the memory object with the specified memory key and maximum length
    memory_obj = ConversationBufferMemory(memory_key="chat_history", max_len=2000)

    # If there are existing conversations for the chat ID, save them to the memory object
    if conversations[chat_id] is not None:
        for hist in conversations[chat_id]:
            memory_obj.save_context(
                {"question": hist["prompt"]},
                {"output": hist["answer"]})

    # Initialize the LLMChain with the OpenAI model, prompt template, and memory object
    chain = LLMChain(
        llm=llm,
        prompt=prompt_template,
        verbose=True,
        memory=memory_obj,
    )

    # Send the input text to the model and return the generated response
    return chain.predict(human_input=input_text)